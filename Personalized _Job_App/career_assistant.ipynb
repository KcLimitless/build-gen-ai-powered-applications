{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Build AI-Powered Q&A bot, Customized cover letter generator app, Resume Polisher app and Career advisor app.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "###Use LLMs with watsonx.ai Locally"
      ],
      "metadata": {
        "id": "9pks6xK9t7M5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ibm-watson-machine-learning Gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1j-f_sEarfmP",
        "outputId": "bc11703e-597a-49a0-9b55-fc6bdab79dcf"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ibm-watson-machine-learning in /usr/local/lib/python3.11/dist-packages (1.0.367)\n",
            "Collecting Gradio\n",
            "  Downloading gradio-5.12.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from ibm-watson-machine-learning) (2.32.2)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.11/dist-packages (from ibm-watson-machine-learning) (2.3.0)\n",
            "Requirement already satisfied: pandas<2.2.0,>=0.24.2 in /usr/local/lib/python3.11/dist-packages (from ibm-watson-machine-learning) (2.1.4)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from ibm-watson-machine-learning) (2024.12.14)\n",
            "Requirement already satisfied: lomond in /usr/local/lib/python3.11/dist-packages (from ibm-watson-machine-learning) (0.3.3)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.11/dist-packages (from ibm-watson-machine-learning) (0.9.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from ibm-watson-machine-learning) (24.2)\n",
            "Requirement already satisfied: ibm-cos-sdk<2.14.0,>=2.12.0 in /usr/local/lib/python3.11/dist-packages (from ibm-watson-machine-learning) (2.13.6)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.11/dist-packages (from ibm-watson-machine-learning) (8.5.0)\n",
            "Collecting aiofiles<24.0,>=22.0 (from Gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from Gradio) (3.7.1)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from Gradio)\n",
            "  Downloading fastapi-0.115.6-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from Gradio)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.5.4 (from Gradio)\n",
            "  Downloading gradio_client-1.5.4-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from Gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.25.1 in /usr/local/lib/python3.11/dist-packages (from Gradio) (0.27.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from Gradio) (3.1.5)\n",
            "Collecting markupsafe~=2.0 (from Gradio)\n",
            "  Downloading MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from Gradio) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from Gradio) (3.10.14)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from Gradio) (11.1.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.11/dist-packages (from Gradio) (2.10.5)\n",
            "Collecting pydub (from Gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from Gradio)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from Gradio) (6.0.2)\n",
            "Collecting ruff>=0.2.2 (from Gradio)\n",
            "  Downloading ruff-0.9.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from Gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from Gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from Gradio)\n",
            "  Downloading starlette-0.45.2-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from Gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from Gradio) (0.15.1)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from Gradio) (4.12.2)\n",
            "Collecting uvicorn>=0.14.0 (from Gradio)\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.5.4->Gradio) (2024.10.0)\n",
            "Requirement already satisfied: websockets<15.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.5.4->Gradio) (14.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->Gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->Gradio) (1.3.1)\n",
            "Collecting starlette<1.0,>=0.40.0 (from Gradio)\n",
            "  Downloading starlette-0.41.3-py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->Gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->Gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.1->Gradio) (3.16.1)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.1->Gradio) (4.67.1)\n",
            "Requirement already satisfied: ibm-cos-sdk-core==2.13.6 in /usr/local/lib/python3.11/dist-packages (from ibm-cos-sdk<2.14.0,>=2.12.0->ibm-watson-machine-learning) (2.13.6)\n",
            "Requirement already satisfied: ibm-cos-sdk-s3transfer==2.13.6 in /usr/local/lib/python3.11/dist-packages (from ibm-cos-sdk<2.14.0,>=2.12.0->ibm-watson-machine-learning) (2.13.6)\n",
            "Requirement already satisfied: jmespath<=1.0.1,>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from ibm-cos-sdk<2.14.0,>=2.12.0->ibm-watson-machine-learning) (1.0.1)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from ibm-cos-sdk-core==2.13.6->ibm-cos-sdk<2.14.0,>=2.12.0->ibm-watson-machine-learning) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<2.2.0,>=0.24.2->ibm-watson-machine-learning) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.11/dist-packages (from pandas<2.2.0,>=0.24.2->ibm-watson-machine-learning) (2024.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->Gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->Gradio) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->ibm-watson-machine-learning) (3.4.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->Gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->Gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->Gradio) (13.9.4)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata->ibm-watson-machine-learning) (3.21.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from lomond->ibm-watson-machine-learning) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->Gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->Gradio) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->Gradio) (0.1.2)\n",
            "Downloading gradio-5.12.0-py3-none-any.whl (57.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.5.4-py3-none-any.whl (321 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m321.4/321.4 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.6-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.9.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m66.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.41.3-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub, uvicorn, tomlkit, semantic-version, ruff, python-multipart, markupsafe, ffmpy, aiofiles, starlette, safehttpx, gradio-client, fastapi, Gradio\n",
            "  Attempting uninstall: markupsafe\n",
            "    Found existing installation: MarkupSafe 3.0.2\n",
            "    Uninstalling MarkupSafe-3.0.2:\n",
            "      Successfully uninstalled MarkupSafe-3.0.2\n",
            "Successfully installed Gradio-5.12.0 aiofiles-23.2.1 fastapi-0.115.6 ffmpy-0.5.0 gradio-client-1.5.4 markupsafe-2.1.5 pydub-0.25.1 python-multipart-0.0.20 ruff-0.9.2 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.41.3 tomlkit-0.13.2 uvicorn-0.34.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ibm_watson_machine_learning.foundation_models import Model\n",
        "from ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams\n",
        "from ibm_watson_machine_learning.foundation_models.utils.enums import ModelTypes, DecodingMethods\n",
        "\n",
        "# Set up the API key and project ID for IBM Watson\n",
        "watsonx_API = \"\"  # Replace with your actual API key\n",
        "project_id = \"\"  # Replace with your actual project ID\n",
        "\n",
        "generate_params = {\n",
        "    GenParams.MAX_NEW_TOKENS: 250\n",
        "}\n",
        "\n",
        "model = Model(\n",
        "    model_id='meta-llama/llama-2-13b-chat',\n",
        "    params=generate_params,\n",
        "    credentials={\n",
        "        \"apikey\": watsonx_API,\n",
        "        # Ensure the URL corresponds to the correct region for your Watsonx instance\n",
        "        \"url\": \"https://eu-de.ml.cloud.ibm.com\"\n",
        "    },\n",
        "    project_id=project_id\n",
        ")\n",
        "\n",
        "q = \"How to be happy?\"\n",
        "generated_response = model.generate(prompt=q)\n",
        "print(generated_response['results'][0]['generated_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1JRskuaw1F0C",
        "outputId": "218a20ee-bc85-4f35-8ec2-146c5fe02a09"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/ibm_watson_machine_learning/foundation_models/utils/utils.py:273: LifecycleWarning: Model 'meta-llama/llama-2-13b-chat' is in deprecated state from 2024-08-26 until None. IDs of alternative models: None. Further details: https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/fm-model-lifecycle.html?context=wx&audience=wdp\n",
            "  warnings.warn(default_warning_template.format(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Happiness is a state of mind that we all strive for, but it can be elusive. Here are some tips to help you find happiness:\n",
            "\n",
            "1. Practice gratitude: Make a conscious effort to focus on the things you are thankful for. Keep a gratitude journal to write down things you are grateful for each day.\n",
            "2. Cultivate positive relationships: Surround yourself with people who support and uplift you. Nurture those relationships by spending quality time with them.\n",
            "3. Take care of your physical health: Regular exercise, healthy eating, and enough sleep can improve your mood and energy levels.\n",
            "4. Engage in activities you enjoy: Do things that bring you joy and fulfillment. Whether it's playing a musical instrument, painting, or hiking, make time for activities that make you happy.\n",
            "5. Practice mindfulness: Focus on the present moment and let go of worries about the past or future. Mindfulness techniques such as meditation and deep breathing can help you stay present and centered.\n",
            "6. Help others: Giving back to your community or helping someone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "rpreNAvy3Bc9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Create a Q&A bot"
      ],
      "metadata": {
        "id": "JWNqE5BM_ozF"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 715
        },
        "id": "7NOZzibQ5Mbl",
        "outputId": "0e69b3b9-b2e3-490c-989a-52c447a06145"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/ibm_watson_machine_learning/foundation_models/utils/utils.py:273: LifecycleWarning: Model 'meta-llama/llama-2-13b-chat' is in deprecated state from 2024-08-26 until None. IDs of alternative models: None. Further details: https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/fm-model-lifecycle.html?context=wx&audience=wdp\n",
            "  warnings.warn(default_warning_template.format(\n",
            "/usr/local/lib/python3.11/dist-packages/gradio/interface.py:403: UserWarning: The `allow_flagging` parameter in `Interface` is deprecated.Use `flagging_mode` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://a47943d86c6b2134d1.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://a47943d86c6b2134d1.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Customized cover letter"
      ],
      "metadata": {
        "id": "ZdL8Njce8o1l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary packages\n",
        "from ibm_watson_machine_learning.foundation_models import Model\n",
        "import gradio as gr\n",
        "\n",
        "# **Verify and update the following values:**\n",
        "watsonx_API = \"\"  # Replace with your actual API key\n",
        "project_id = \"\"  # Replace with your actual project ID\n",
        "\n",
        "# Model and project settings\n",
        "model_id = \"meta-llama/llama-2-13b-chat\"  # Directly specifying the LLAMA2 model\n",
        "\n",
        "# Set credentials to use the model\n",
        "my_credentials = {\n",
        "    \"apikey\": watsonx_API,\n",
        "    \"url\": \"https://eu-de.ml.cloud.ibm.com\"\n",
        "}\n",
        "\n",
        "# Generation parameters\n",
        "gen_parms = {\n",
        "    \"max_new_tokens\": 512,  # Adjust as needed for the length of the cover letter\n",
        "    \"temperature\": 0.7  # Adjust for creativity\n",
        "}\n",
        "#project_id = \"skills-network\"\n",
        "space_id = None\n",
        "verify = False\n",
        "\n",
        "# Initialize the model\n",
        "model = Model(model_id, my_credentials, gen_parms, project_id, space_id, verify)\n",
        "\n",
        "# Function to generate a customized cover letter\n",
        "def generate_cover_letter(company_name, position_name, job_description, resume_content):\n",
        "    # Craft the prompt for the model to generate a cover letter\n",
        "    prompt = f\"Generate a customized cover letter using the company name: {company_name}, the position applied for: {position_name}, and the job description: {job_description}. Ensure the cover letter highlights my qualifications and experience as detailed in the resume content: {resume_content}. Adapt the content carefully to avoid including experiences not present in my resume but mentioned in the job description. The goal is to emphasize the alignment between my existing skills and the requirements of the role.\"\n",
        "\n",
        "    generated_response = model.generate(prompt, gen_parms)\n",
        "\n",
        "    # Extract the generated text\n",
        "    cover_letter = generated_response[\"results\"][0][\"generated_text\"]\n",
        "    return cover_letter\n",
        "\n",
        "# Create Gradio interface for the cover letter generation application\n",
        "cover_letter_app = gr.Interface(\n",
        "    fn=generate_cover_letter,\n",
        "    allow_flagging=\"never\", # Deactivate the flag function in gradio as it is not needed.\n",
        "    inputs=[\n",
        "        gr.Textbox(label=\"Company Name\", placeholder=\"Enter the name of the company...\"),\n",
        "        gr.Textbox(label=\"Position Name\", placeholder=\"Enter the name of the position...\"),\n",
        "        gr.Textbox(label=\"Job Description Information\", placeholder=\"Paste the job description here...\", lines=10),\n",
        "        gr.Textbox(label=\"Resume Content\", placeholder=\"Paste your resume content here...\", lines=10),\n",
        "    ],\n",
        "    outputs=gr.Textbox(label=\"Customized Cover Letter\"),\n",
        "    title=\"Customized Cover Letter Generator\",\n",
        "    description=\"Generate a customized cover letter by entering the company name, position name, job description and your resume.\"\n",
        ")\n",
        "\n",
        "# Launch the application\n",
        "cover_letter_app.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 715
        },
        "id": "ibkEgo848sgr",
        "outputId": "d8a094c7-c5ea-4bf7-9242-bf518684cf46"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/ibm_watson_machine_learning/foundation_models/utils/utils.py:273: LifecycleWarning: Model 'meta-llama/llama-2-13b-chat' is in deprecated state from 2024-08-26 until None. IDs of alternative models: None. Further details: https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/fm-model-lifecycle.html?context=wx&audience=wdp\n",
            "  warnings.warn(default_warning_template.format(\n",
            "/usr/local/lib/python3.11/dist-packages/gradio/interface.py:403: UserWarning: The `allow_flagging` parameter in `Interface` is deprecated.Use `flagging_mode` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://1f097954760ee0f6c3.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://1f097954760ee0f6c3.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Resume polisher"
      ],
      "metadata": {
        "id": "FkddESop8gOp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary packages\n",
        "from ibm_watson_machine_learning.foundation_models import Model\n",
        "import gradio as gr\n",
        "\n",
        "# **Verify and update the following values:**\n",
        "watsonx_API = \"\"  # Replace with your actual API key\n",
        "project_id = \"\"  # Replace with your actual project ID\n",
        "\n",
        "\n",
        "model_id = \"meta-llama/llama-2-13b-chat\"  # Directly specifying the LLAMA2 model\n",
        "\n",
        "# Set credentials to use the model\n",
        "my_credentials = {\n",
        "    \"apikey\": watsonx_API,\n",
        "    \"url\": \"https://eu-de.ml.cloud.ibm.com\"\n",
        "}\n",
        "\n",
        "# Generation parameters\n",
        "gen_parms = {\n",
        "    \"max_new_tokens\": 512,  # Increased token limit for larger content\n",
        "    \"temperature\": 0.7  # Adjusted for more creative variations\n",
        "}\n",
        "#project_id = \"skills-network\"  # Specifying project_id as provided\n",
        "space_id = None\n",
        "verify = False\n",
        "\n",
        "# Initialize the model\n",
        "model = Model(model_id, my_credentials, gen_parms, project_id, space_id, verify)\n",
        "\n",
        "# Function to polish the resume using the model, making polish_prompt optional\n",
        "def polish_resume(position_name, resume_content, polish_prompt=\"\"):\n",
        "    # Check if polish_prompt is provided and adjust the combined_prompt accordingly\n",
        "    if polish_prompt and polish_prompt.strip():\n",
        "        prompt_use = f\"Given the resume content: '{resume_content}', polish it based on the following instructions: {polish_prompt} for the {position_name} position.\"\n",
        "    else:\n",
        "        prompt_use = f\"Suggest improvements for the following resume content: '{resume_content}' to better align with the requirements and expectations of a {position_name} position. Return the polished version, highlighting necessary adjustments for clarity, relevance, and impact in relation to the targeted role.\"\n",
        "\n",
        "    # Generate a response using the model with parameters\n",
        "    generated_response = model.generate(prompt_use)\n",
        "\n",
        "    # Extract and return the generated text\n",
        "    generated_text = generated_response[\"results\"][0][\"generated_text\"]\n",
        "    return generated_text\n",
        "\n",
        "# Create Gradio interface for the resume polish application, marking polish_prompt as optional\n",
        "resume_polish_application = gr.Interface(\n",
        "    fn=polish_resume,\n",
        "    allow_flagging=\"never\", # Deactivate the flag function in gradio as it is not needed.\n",
        "    inputs=[\n",
        "        gr.Textbox(label=\"Position Name\", placeholder=\"Enter the name of the position...\"),\n",
        "        gr.Textbox(label=\"Resume Content\", placeholder=\"Paste your resume content here...\", lines=20),\n",
        "        gr.Textbox(label=\"Polish Instruction (Optional)\", placeholder=\"Enter specific instructions or areas for improvement (optional)...\", lines=2),\n",
        "    ],\n",
        "    outputs=gr.Textbox(label=\"Polished Content\"),\n",
        "    title=\"Resume Polish Application\",\n",
        "    description=\"This application helps you polish your resume. Enter the position your want to apply, your resume content, and specific instructions or areas for improvement (optional), then get a polished version of your content.\"\n",
        ")\n",
        "\n",
        "# Launch the application\n",
        "resume_polish_application.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 715
        },
        "id": "2ZIKuQXL5MZn",
        "outputId": "4aa5d8eb-c91e-4856-f08c-9380fefe19cf"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/ibm_watson_machine_learning/foundation_models/utils/utils.py:273: LifecycleWarning: Model 'meta-llama/llama-2-13b-chat' is in deprecated state from 2024-08-26 until None. IDs of alternative models: None. Further details: https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/fm-model-lifecycle.html?context=wx&audience=wdp\n",
            "  warnings.warn(default_warning_template.format(\n",
            "/usr/local/lib/python3.11/dist-packages/gradio/interface.py:403: UserWarning: The `allow_flagging` parameter in `Interface` is deprecated.Use `flagging_mode` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://749ff829128ea79e11.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://749ff829128ea79e11.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Career advisor"
      ],
      "metadata": {
        "id": "ASOPXO4_96eC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary packages\n",
        "from ibm_watson_machine_learning.foundation_models import Model\n",
        "import gradio as gr\n",
        "\n",
        "# **Verify and update the following values:**\n",
        "watsonx_API = \"\"  # Replace with your actual API key\n",
        "project_id = \"\"  # Replace with your actual project ID\n",
        "\n",
        "# Model and project settings\n",
        "model_id = \"meta-llama/llama-2-13b-chat\"  # Directly specifying the LLAMA2 model\n",
        "\n",
        "# Set credentials\n",
        "my_credentials = {\n",
        "    \"apikey\": watsonx_API,\n",
        "    \"url\": \"https://eu-de.ml.cloud.ibm.com\"\n",
        "}\n",
        "\n",
        "# Generation parameters\n",
        "gen_parms = {\n",
        "    \"max_new_tokens\": 1024,  # Adjust as needed\n",
        "    \"temperature\": 0.7  # Adjust for creativity\n",
        "}\n",
        "\n",
        "#project_id = \"skills-network\"  # Use this project_id\n",
        "space_id = None\n",
        "verify = False\n",
        "\n",
        "# Initialize the model with the correct parameters\n",
        "model = Model(model_id, my_credentials, gen_parms, project_id, space_id, verify)\n",
        "\n",
        "# Function to generate career advice\n",
        "def generate_career_advice(position_applied, job_description, resume_content):\n",
        "    # The prompt for the model\n",
        "    prompt = f\"Considering the job description: {job_description}, and the resume provided: {resume_content}, identify areas for enhancement in the resume. Offer specific suggestions on how to improve these aspects to better match the job requirements and increase the likelihood of being selected for the position of {position_applied}.\"\n",
        "\n",
        "    # Generate response\n",
        "    generated_response = model.generate(prompt, gen_parms)\n",
        "\n",
        "    # Extract and format the generated text\n",
        "    advice = generated_response[\"results\"][0][\"generated_text\"]\n",
        "    return advice\n",
        "\n",
        "# Create Gradio interface for the career advice application\n",
        "career_advice_app = gr.Interface(\n",
        "    fn=generate_career_advice,\n",
        "    allow_flagging=\"never\", # Deactivate the flag function in gradio as it is not needed.\n",
        "    inputs=[\n",
        "        gr.Textbox(label=\"Position Applied For\", placeholder=\"Enter the position you are applying for...\"),\n",
        "        gr.Textbox(label=\"Job Description Information\", placeholder=\"Paste the job description here...\", lines=10),\n",
        "        gr.Textbox(label=\"Your Resume Content\", placeholder=\"Paste your resume content here...\", lines=10),\n",
        "    ],\n",
        "    outputs=gr.Textbox(label=\"Advice\"),\n",
        "    title=\"Career Advisor\",\n",
        "    description=\"Enter the position you're applying for, paste the job description, and your resume content to get advice on what to improve for getting this job.\"\n",
        ")\n",
        "\n",
        "# Launch the application\n",
        "career_advice_app.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 715
        },
        "id": "maHEgGZ298c7",
        "outputId": "3de7f734-e710-464e-d4d3-c565af5d2252"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/ibm_watson_machine_learning/foundation_models/utils/utils.py:273: LifecycleWarning: Model 'meta-llama/llama-2-13b-chat' is in deprecated state from 2024-08-26 until None. IDs of alternative models: None. Further details: https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/fm-model-lifecycle.html?context=wx&audience=wdp\n",
            "  warnings.warn(default_warning_template.format(\n",
            "/usr/local/lib/python3.11/dist-packages/gradio/interface.py:403: UserWarning: The `allow_flagging` parameter in `Interface` is deprecated.Use `flagging_mode` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://8c75c01a40810bb3c5.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://8c75c01a40810bb3c5.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    }
  ]
}