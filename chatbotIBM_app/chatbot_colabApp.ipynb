{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Create a Q&A bot\n",
        "\n",
        "Today we will be building a simple conversational chatbot with an interesting generative model. And we will be deploying it with gradio and hugging face spaces: facebook/blenderbot-400M-distill is a lightweight, distilled version of Facebook AI's BlenderBot model, designed for open-domain conversational AI. It has 400 million parameters and is optimized for efficiency while maintaining good performance in generating human-like, context-aware responses. The model is ideal for chatbots and dialogue systems, offering capabilities for casual conversation, answering questions, and engaging in meaningful interactions. It is available via the Hugging Face Transformers library for easy integration into applications.\n",
        "\n",
        "**Note:** Please note that the model used in this project is a basic, lightweight version, not intended for handling complex queries. For more advanced and robust LLMs, you can explore a wide range of options at huggingface.com."
      ],
      "metadata": {
        "id": "X51Xvd1oTKHL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Installing necessary libraries"
      ],
      "metadata": {
        "id": "nwwnnVCQS3r1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio transformers huggingface_hub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3uKKks5egNn",
        "outputId": "cd8f88c0-b66b-403e-aa95-a726917a7b65"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-5.12.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.1)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.27.1)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.6-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.5.4 (from gradio)\n",
            "  Downloading gradio_client-1.5.4-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.5)\n",
            "Collecting markupsafe~=2.0 (from gradio)\n",
            "  Downloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.10.13)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (11.1.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.10.4)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.2.2 (from gradio)\n",
            "  Downloading ruff-0.9.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.45.2-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.15.1)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.5.4->gradio) (2024.10.0)\n",
            "Requirement already satisfied: websockets<15.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.5.4->gradio) (14.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.5.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.41.3-py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.27.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-5.12.0-py3-none-any.whl (57.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 MB\u001b[0m \u001b[31m40.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.5.4-py3-none-any.whl (321 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m321.4/321.4 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.6-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.9.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m122.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.41.3-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub, uvicorn, tomlkit, semantic-version, ruff, python-multipart, markupsafe, ffmpy, aiofiles, starlette, safehttpx, gradio-client, fastapi, gradio\n",
            "  Attempting uninstall: markupsafe\n",
            "    Found existing installation: MarkupSafe 3.0.2\n",
            "    Uninstalling MarkupSafe-3.0.2:\n",
            "      Successfully uninstalled MarkupSafe-3.0.2\n",
            "Successfully installed aiofiles-23.2.1 fastapi-0.115.6 ffmpy-0.5.0 gradio-5.12.0 gradio-client-1.5.4 markupsafe-2.1.5 pydub-0.25.1 python-multipart-0.0.20 ruff-0.9.1 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.41.3 tomlkit-0.13.2 uvicorn-0.34.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##First implementation with Gradio interface (gr.Interface())"
      ],
      "metadata": {
        "id": "UcNwS_M4_roQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gradio as gr\n",
        "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
        "from huggingface_hub import login\n",
        "\n",
        "# Load model directly\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# Replace with your Hugging Face token\n",
        "HF_ACCESS_TOKEN = \"Hugging Face token\"\n",
        "\n",
        "# Authenticate\n",
        "login(token=HF_ACCESS_TOKEN)\n",
        "\n",
        "# Specify the model name\n",
        "model_name = \"facebook/blenderbot-400M-distill\"\n",
        "\n",
        "\n",
        "# Load the tokenizer and model locally\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Generate a response\n",
        "def generate_response(prompt_txt):\n",
        "    try:\n",
        "        # Encode the input text\n",
        "        inputs = tokenizer(prompt_txt, return_tensors=\"pt\")\n",
        "\n",
        "        # Generate a response using the model\n",
        "        outputs = model.generate(**inputs, max_new_tokens=250, temperature=0.5)\n",
        "\n",
        "        # Decode the generated text\n",
        "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        return generated_text\n",
        "    except Exception as e:\n",
        "        return f\"Error generating response: {e}\"\n",
        "\n",
        "# Example usage\n",
        "#prompt = \"What is the capital of France?\"\n",
        "#response = generate_response(prompt)\n",
        "#print(response)\n",
        "\n",
        "# Create Gradio interface\n",
        "chat_application = gr.Interface(\n",
        "    fn=generate_response,\n",
        "    allow_flagging=\"never\",\n",
        "    inputs=gr.Textbox(label=\"Input\", lines=2, placeholder=\"Type your question here...\"),\n",
        "    outputs=gr.Textbox(label=\"Output\"),\n",
        "    title=\"blenderbot-400M-distill\",\n",
        "    description=\"Ask any question and the chatbot will try to answer.\"\n",
        ")\n",
        "\n",
        "# Launch Gradio app\n",
        "chat_application.launch(share=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "vy0rq4bCfK24",
        "outputId": "80a168d3-b203-4b70-f826-7cfb87cd7914"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gradio/interface.py:403: UserWarning: The `allow_flagging` parameter in `Interface` is deprecated.Use `flagging_mode` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://60ff6c327973c0c125.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://60ff6c327973c0c125.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "prompt = \"What is the capital of France?\"\n",
        "response = generate_response(prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QDFvhgJ3bs9Q",
        "outputId": "a8db8f3a-6154-479e-80db-5840cb1c06a0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " The capital is Paris. It is the most populous city in the French Republic.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Second implementation with Gradio Block (gr.Block())"
      ],
      "metadata": {
        "id": "8ZfFOQYzAfV3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import gradio as gr\n",
        "\n",
        "# Choosing a model\n",
        "model_name = \"facebook/blenderbot-400M-distill\"\n",
        "\n",
        "# Fetch the model and initialize a tokenizer\n",
        "# Load model (download on first run and reference local installation for consequent runs)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Keeping track of conversation history\n",
        "conversation_history = []\n",
        "\n",
        "# Function to generate a chatbot response\n",
        "def chatbot_response(user_input):\n",
        "    global conversation_history\n",
        "\n",
        "    # Create conversation history string\n",
        "    history_string = \"\\n\".join(conversation_history)\n",
        "\n",
        "    # Tokenize the input text and history\n",
        "    inputs = tokenizer.encode_plus(history_string, user_input, return_tensors=\"pt\")\n",
        "\n",
        "    # Generate the response from the model\n",
        "    outputs = model.generate(**inputs)\n",
        "\n",
        "    # Decode the response\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
        "\n",
        "    # Add interaction to conversation history\n",
        "    conversation_history.append(user_input)\n",
        "    conversation_history.append(response)\n",
        "\n",
        "    return response\n",
        "\n",
        "# Function to reset the conversation history\n",
        "def reset_conversation():\n",
        "    global conversation_history\n",
        "    conversation_history = []\n",
        "    return \"Conversation reset. How can I assist you?\"\n",
        "\n",
        "\n",
        "# Create Gradio interface using with gr.Blocks() to manage the context:\n",
        "with gr.Blocks() as interface:  # Create a Blocks context\n",
        "    # Define input and output components\n",
        "    input_box = gr.Textbox(label=\"Your Message\", placeholder=\"Type your question here...\")\n",
        "    output_box = gr.Textbox(label=\"Bot Response\")\n",
        "\n",
        "    # Create a submit button\n",
        "    submit_button = gr.Button(\"Submit\")\n",
        "\n",
        "    # Create a reset button\n",
        "    reset_button = gr.Button(\"Reset Conversation\")\n",
        "\n",
        "    # Link the submit button to the chatbot_response function\n",
        "    submit_button.click(fn=chatbot_response, inputs=input_box, outputs=output_box)\n",
        "\n",
        "    # Link the reset button to the reset function\n",
        "    reset_button.click(fn=reset_conversation, outputs=output_box)\n",
        "\n",
        "# Set title and description\n",
        "interface.title = \"BlenderBot Chatbot\"\n",
        "interface.description = \"A conversational AI chatbot powered by BlenderBot. Ask anything!\"\n",
        "interface.examples = [\n",
        "    [\"Hello!\"],\n",
        "    [\"Can you tell me a joke?\"],\n",
        "    [\"What is the capital of France?\"],\n",
        "]\n",
        "interface.live = True\n",
        "\n",
        "# Launch the Gradio app (for local testing)\n",
        "if __name__ == \"__main__\":\n",
        "    interface.launch()\n",
        "\n",
        "# For deployment on Hugging Face Spaces, save this script as `app.py` and push to your Hugging Face Space repository."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "w5COwKgdjBdA",
        "outputId": "410dac33-1680-4eb4-acb6-7349d79846f0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://863b52b0ae2b85c755.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://863b52b0ae2b85c755.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Third implementation with Gradio Block (gr.Block()) and integrating chat history"
      ],
      "metadata": {
        "id": "PDOL-AciA24F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import gradio as gr\n",
        "\n",
        "# Choosing a model\n",
        "model_name = \"facebook/blenderbot-400M-distill\"\n",
        "\n",
        "# Fetch the model and initialize a tokenizer\n",
        "# Load model (download on first run and reference local installation for consequent runs)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Keeping track of conversation history\n",
        "conversation_history = []\n",
        "\n",
        "# Function to generate a chatbot response\n",
        "def chatbot_response(user_input):\n",
        "    global conversation_history\n",
        "\n",
        "    # Create conversation history string\n",
        "    history_string = \"\\n\".join([msg for sender, msg in conversation_history if sender == \"Bot\"]) # Join bot messages for context\n",
        "\n",
        "    # Tokenize the input text and history\n",
        "    inputs = tokenizer.encode_plus(history_string, user_input, return_tensors=\"pt\")\n",
        "\n",
        "    # Generate the response from the model\n",
        "    outputs = model.generate(**inputs)\n",
        "\n",
        "    # Decode the response\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
        "\n",
        "    # Add interaction to conversation history as a list of [sender, message]\n",
        "    conversation_history.append([\"User\", user_input])  # Changed to a list\n",
        "    conversation_history.append([\"Bot\", response])    # Changed to a list\n",
        "\n",
        "    # Update the chat history directly within the function\n",
        "    return response, gr.update(value=conversation_history) # Update chat_history with conversation history\n",
        "\n",
        "\n",
        "# Function to reset the conversation history\n",
        "def reset_conversation():\n",
        "    global conversation_history\n",
        "    conversation_history = []\n",
        "    # Update chat history to reflect reset\n",
        "    return \"Conversation reset. How can I assist you?\", gr.update(value=[])  # Empty list for reset\n",
        "\n",
        "# Create Gradio interface using with gr.Blocks() to manage the context:\n",
        "with gr.Blocks() as interface:  # Create a Blocks context\n",
        "    # Define input and output components\n",
        "    input_box = gr.Textbox(label=\"Your Message\", placeholder=\"Type your question here...\")\n",
        "    output_box = gr.Textbox(label=\"Bot Response\")\n",
        "\n",
        "    # Create a chat history display\n",
        "    chat_history = gr.Chatbot(label=\"Chat History\", show_label=False, height=400) # Changed gr.Chatbox to gr.Chatbot, removed show_line_numbers, and changed lines to height\n",
        "\n",
        "    # Create submit button\n",
        "    submit_button = gr.Button(\"Submit\")\n",
        "\n",
        "    # Create reset button\n",
        "    reset_button = gr.Button(\"Reset Conversation\")\n",
        "\n",
        "    # Link the submit button to the chatbot_response function\n",
        "    submit_button.click(fn=chatbot_response, inputs=input_box, outputs=[output_box, chat_history]) # outputs is now a list\n",
        "\n",
        "    # Link the reset button to the reset function\n",
        "    reset_button.click(fn=reset_conversation, outputs=[output_box, chat_history]) # outputs is now a list\n",
        "\n",
        "\n",
        "# Set title and description\n",
        "interface.title = \"BlenderBot Chatbot\"\n",
        "interface.description = \"A conversational AI chatbot powered by BlenderBot. Ask anything!\"\n",
        "interface.examples = [\n",
        "    [\"Hello!\"],\n",
        "    [\"Can you tell me a joke?\"],\n",
        "    [\"What is the capital of France?\"],\n",
        "]\n",
        "interface.live = True\n",
        "\n",
        "# Launch the Gradio app (for local testing)\n",
        "if __name__ == \"__main__\":\n",
        "    interface.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "id": "8VQShN_do_Yk",
        "outputId": "f6576bd5-d54e-402e-ea07-565375bcb4f7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gradio/components/chatbot.py:273: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://68f2bc7c3af3eebe71.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://68f2bc7c3af3eebe71.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ZnzhWGKqrPA7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fastapi uvicorn nest-asyncio pyngrok transformers torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ElaUoj7rQUV",
        "outputId": "224dfd9c-b9d7-4ee4-e913-58d5daf17ff3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fastapi\n",
            "  Downloading fastapi-0.115.6-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting uvicorn\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (1.6.0)\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.3-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Collecting starlette<0.42.0,>=0.40.0 (from fastapi)\n",
            "  Downloading starlette-0.41.3-py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from fastapi) (2.10.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from fastapi) (4.12.2)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (8.1.8)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (0.14.0)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.5.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.27.2)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette<0.42.0,>=0.40.0->fastapi) (3.7.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.12.14)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.42.0,>=0.40.0->fastapi) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.42.0,>=0.40.0->fastapi) (1.2.2)\n",
            "Downloading fastapi-0.115.6-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyngrok-7.2.3-py3-none-any.whl (23 kB)\n",
            "Downloading starlette-0.41.3-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: uvicorn, pyngrok, starlette, fastapi\n",
            "Successfully installed fastapi-0.115.6 pyngrok-7.2.3 starlette-0.41.3 uvicorn-0.34.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Fourth implementation with FastAPI"
      ],
      "metadata": {
        "id": "FM185cRkBzE5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from fastapi import FastAPI, Request\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from pydantic import BaseModel\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "import nest_asyncio\n",
        "from pyngrok import ngrok\n",
        "import uvicorn\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# **Replace \"YOUR_AUTHTOKEN\" with your actual ngrok authtoken**\n",
        "ngrok.set_auth_token(\"YOUR_AUTHTOKEN\")\n",
        "\n",
        "# Start ngrok tunnel\n",
        "ngrok_tunnel = ngrok.connect(8000)\n",
        "print('Public URL:', ngrok_tunnel.public_url)\n",
        "\n",
        "\n",
        "# Initialize FastAPI app\n",
        "app = FastAPI()\n",
        "\n",
        "# Add CORS middleware\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],  # Adjust origins as needed for production\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "# Model setup\n",
        "model_name = \"facebook/blenderbot-400M-distill\"\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# In-memory conversation history\n",
        "conversation_history = []\n",
        "\n",
        "\n",
        "# Request schema using Pydantic\n",
        "class ChatRequest(BaseModel):\n",
        "    prompt: str\n",
        "\n",
        "\n",
        "# Root route\n",
        "@app.get(\"/\")\n",
        "async def root():\n",
        "    \"\"\"\n",
        "    Handle requests to the root path.\n",
        "    \"\"\"\n",
        "    return {\"message\": \"Welcome to the BlenderBot chatbot! Send your requests to /chatbot\"}\n",
        "\n",
        "\n",
        "@app.post(\"/chatbot\")\n",
        "async def handle_prompt(request: ChatRequest):\n",
        "    \"\"\"\n",
        "    Handle chat requests by generating responses using the model.\n",
        "    \"\"\"\n",
        "    input_text = request.prompt\n",
        "\n",
        "    # Create conversation history string\n",
        "    history = \"\\n\".join(conversation_history)\n",
        "\n",
        "    # Tokenize the input text and history\n",
        "    inputs = tokenizer.encode_plus(history, input_text, return_tensors=\"pt\")\n",
        "\n",
        "    # Generate the response from the model\n",
        "    outputs = model.generate(**inputs, max_length=60)\n",
        "\n",
        "    # Decode the response\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
        "\n",
        "    # Add interaction to conversation history\n",
        "    conversation_history.append(input_text)\n",
        "    conversation_history.append(response)\n",
        "\n",
        "    return {\"response\": response}\n",
        "\n",
        "\n",
        "# Run the FastAPI app\n",
        "uvicorn.run(app, port=8000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mFveCX1wx_Zy",
        "outputId": "40ca1742-6014-4c6d-bfe5-34863a6d9399"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Public URL: https://1fec-35-201-240-180.ngrok-free.app\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Started server process [2815]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n",
            "Process Process-auto_conversion:\n",
            "INFO:     Shutting down\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/safetensors_conversion.py\", line 84, in auto_conversion\n",
            "    sha = get_conversion_pr_reference(api, pretrained_model_name_or_path, **cached_file_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/safetensors_conversion.py\", line 68, in get_conversion_pr_reference\n",
            "    pr = previous_pr(api, model_id, pr_title, token=token)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/safetensors_conversion.py\", line 16, in previous_pr\n",
            "    commits = api.list_repo_commits(model_id, revision=discussion.git_reference, token=token)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/hf_api.py\", line 3216, in list_repo_commits\n",
            "    return [\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/hf_api.py\", line 3216, in <listcomp>\n",
            "    return [\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_pagination.py\", line 36, in paginate\n",
            "    r = session.get(path, params=params, headers=headers)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/requests/sessions.py\", line 602, in get\n",
            "    return self.request(\"GET\", url, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/requests/sessions.py\", line 589, in request\n",
            "    resp = self.send(prep, **send_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/requests/sessions.py\", line 703, in send\n",
            "    r = adapter.send(request, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_http.py\", line 93, in send\n",
            "    return super().send(request, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/requests/adapters.py\", line 667, in send\n",
            "    resp = conn.urlopen(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\", line 787, in urlopen\n",
            "    response = self._make_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\", line 534, in _make_request\n",
            "    response = conn.getresponse()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/connection.py\", line 516, in getresponse\n",
            "    httplib_response = super().getresponse()\n",
            "  File \"/usr/lib/python3.10/http/client.py\", line 1375, in getresponse\n",
            "    response.begin()\n",
            "  File \"/usr/lib/python3.10/http/client.py\", line 318, in begin\n",
            "    version, status, reason = self._read_status()\n",
            "  File \"/usr/lib/python3.10/http/client.py\", line 279, in _read_status\n",
            "    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n",
            "  File \"/usr/lib/python3.10/socket.py\", line 705, in readinto\n",
            "    return self._sock.recv_into(b)\n",
            "  File \"/usr/lib/python3.10/ssl.py\", line 1303, in recv_into\n",
            "    return self.read(nbytes, buffer)\n",
            "  File \"/usr/lib/python3.10/ssl.py\", line 1159, in read\n",
            "    return self._sslobj.read(len, buffer)\n",
            "KeyboardInterrupt\n",
            "INFO:     Waiting for application shutdown.\n",
            "INFO:     Application shutdown complete.\n",
            "INFO:     Finished server process [2815]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After running the field above, you should see a public URL in the output(e.g., http://<ngrok_id>.ngrok.io).\n",
        "- Use the public URL to access the app.\n",
        "- Append /docs to view the auto-generated Swagger UI (e.g., http://<ngrok_id>.ngrok.io/docs).\n",
        "- Use the /chatbot endpoint in the Swagger UI or Postman to test the chatbot.\n",
        "- Alternatively use curl to query the chatbot as seen below."
      ],
      "metadata": {
        "id": "bCsX7qWCCQjd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -X POST -H \"Content-Type: application/json\" -d '{\"prompt\": \"Hello, how are you?\"}' https://be32-35-229-155-233.ngrok-free.app/chatbot"
      ],
      "metadata": {
        "id": "rOZEhW7C2mSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "{\"response\":\"Hello Hello Hello,, I am a a hell hell hell... I am good..\"}"
      ],
      "metadata": {
        "id": "C9K0vUBZ2qJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -X POST \\\n",
        "  \"https://8edb-35-201-240-180.ngrok-free.app/chatbot\" \\\n",
        "  -H \"accept: application/json\" \\\n",
        "  -H \"Content-Type: application/json\" \\\n",
        "  -d '{\"prompt\": \"What is the capital of France?\"}'\n"
      ],
      "metadata": {
        "id": "GF6CZl7zGqaY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "{\n",
        "  \"response\": \"I am good. I am from france. The capital of france is paris.\"\n",
        "}"
      ],
      "metadata": {
        "id": "O4IyhDnMGwaV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -X POST \\\n",
        "  'https://8edb-35-201-240-180.ngrok-free.app/chatbot' \\\n",
        "  -H 'accept: application/json' \\\n",
        "  -H 'Content-Type: application/json' \\\n",
        "  -d '{\"prompt\": \"Can you tell me a joke?\"}'"
      ],
      "metadata": {
        "id": "yf4BbwXmILeb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}